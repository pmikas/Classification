{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:30px\">Model Training - Classification</h1>\n",
    "<hr>\n",
    "\n",
    "1. Split the dataset\n",
    "2. Build model pipelines\n",
    "3. Declare hyperparameters o tune\n",
    "4. Fit and tune models with cross-validation\n",
    "5. Evaluate metrics and select winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**Import libraries**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for Dataframes\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns',100)\n",
    "\n",
    "# Matplolib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn for modeling\n",
    "import sklearn\n",
    "\n",
    "# Pickle for saving model files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Function for standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import RandomForestClassifier and GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**Load analytical base table**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analytical base table from Feature Engineering\n",
    "df = pd.read_csv('analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**1. Split the dataset**</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the dataframe into separate objects for the target variable (y) and the input features (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.class_p\n",
    "\n",
    "# Create separate object for input features\n",
    "X = df.drop('class_p', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training sets** are used to fit and tune the models<br>\n",
    "**Test sets** are put aside as unseen data to evaluate your models\n",
    "<br>\n",
    "* Split the train and test set, passing in the argument **test_size = 0.2** to set aside 20% of the observations for the test set\n",
    "* The **random_state = 1234** is set for replicable results\n",
    "* **Important**: For classification model also pass in the argument **stratify = df.target** in order to make sure the target variable's classes are balanced in each subset of data. This is **stratified random sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6499 1625 6499 1625\n"
     ]
    }
   ],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**2. Build model pipelines**</span><br>\n",
    "The pipeline will standardize the data first, then apply the model algorithm to it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**: should be performed inside the cross-validation loop\n",
    "* Transform or scale the features\n",
    "* Perform automatic feature reduction (e.g. PCA)\n",
    "* Remove correlated features<br>\n",
    "<br>\n",
    "**Standartization**: transforms all features to the same **scale** by substracting means and dividing by standard deviations.\n",
    "* Feature's distribution **centered around zero, with unit variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **random_state = 123** is set for replicable results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification pipelines**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines dictionary\n",
    "pipelines = {'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l1', random_state = 123)),\n",
    "             'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty = 'l2', random_state = 123)),\n",
    "             'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state = 123)),\n",
    "             'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state = 123))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**3. Declare hyperparameters to tune**</span><br>\n",
    "**Hyperparameters** express \"higher-level\" structural settings for modeling algorithms<br>\n",
    "* e.g. strength of the penalty used in regularized regression\n",
    "* e.g. the number of trees to include in a random forest\n",
    "* They are **decided** before training the model because they cannot be learned from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **C** is the **strength of the penalty**, inverse of alpha from regularization strength\n",
    "* Higher values of C mean weacker penalties\n",
    "* C is a positive value, typically between 0 and 1000\n",
    "* The default is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameters\n",
    "l1_hyperparameters = {'logisticregression__C': np.linspace(1e-3, 1e3, 10)}\n",
    "l2_hyperparameters = {'logisticregression__C': np.linspace(1e-3, 1e3, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = {'randomforestclassifier__n_estimators': [100, 200],\n",
    "                      'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted tree hyperparameters\n",
    "gb_hyperparameters = {'gradientboostingclassifier__n_estimators': [100, 200],\n",
    "                      'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "                      'gradientboostingclassifier__max_depth': [1, 3, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameters\n",
    "hyperparameters = {'l1': l1_hyperparameters,\n",
    "                   'l2': l2_hyperparameters,\n",
    "                   'rf': rf_hyperparameters,\n",
    "                   'gb': gb_hyperparameters}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**4. Fit and tune models with cross-validation**</span><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV function performs cross-validation on the **hyperparameter grid**, through each **combination of values**. It then calculates **cross-validated scores** (using performance metrics) for each combination of hyperparameter values and picks the combination that has the best score\n",
    "* **cv** is the number of cross-validation folds\n",
    "* **n_jobs = -1** trains in parallel across the maximum number of cores of the computer, speeding it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 has been fitted.\n",
      "l2 has been fitted.\n",
      "rf has been fitted.\n",
      "gb has been fitted.\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    \n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv = 10, n_jobs = -1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name]\n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print when the model is fitted\n",
    "    print(name, 'has been fitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:18px\">**5. Evaluate models and select winner**</span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 0.9998461301738729\n",
      "l2 0.9998461301738729\n",
      "rf 1.0\n",
      "gb 0.9998461301738729\n"
     ]
    }
   ],
   "source": [
    "# Display best_score_ for each fitted_model\n",
    "for name, model in fitted_models.items():\n",
    "    print(name, model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classification metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First evaluate the models by looking at their **cross-validated performance** on the training set, through the **holdout accuracy** scores\n",
    "* Accuracy is simply the percent of observations correctly classified by the model\n",
    "* Because is the average accuracy from the **holdout folds**, higher is almost always better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straight accuracy is not always the best way to evaluate a classification model, specifically when evaluating **imbalanced classes** in the target variable<br>\n",
    "<br>\n",
    "**Area under ROC curve** is the most reliable metric for classification tasks. It is equivalent to the probability that a randomly chosen **positive** observation ranks higher (has a higher predicted probability) than a randomly chosen **negative** observation\n",
    "* ROC curve is a way to visualize the **relationship between TPR (true positive rate) and FPR (false positive rate)** for classification models\n",
    "* Plot the TPR and FPR at different **thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1 1.0\n",
      "l2 1.0\n",
      "rf 1.0\n",
      "gb 1.0\n"
     ]
    }
   ],
   "source": [
    "# Loop through the fitted_models to predict probabilities\n",
    "for name, model in fitted_models.items():\n",
    "    pred = model.predict_proba(X_test)\n",
    "    \n",
    "    # Get just the prediction for the positive class (1)\n",
    "    pred = [p[1] for p in pred]\n",
    "    \n",
    "    # Calculate ROC curve from y_test and pred\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "    \n",
    "    # Calculate and print AUROC\n",
    "    print(name, auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=111.11200000000001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l1', random_state=123,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('logisticregression', LogisticRegression(C=111.11200000000001, class_weight=None, dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=123,\n",
      "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestclassifier', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min...imators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=123, verbose=0, warm_start=False))])\n",
      "Pipeline(memory=None,\n",
      "     steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('gradientboostingclassifier', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.05, loss='deviance', max_depth=5,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "  ...       presort='auto', random_state=123, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n"
     ]
    }
   ],
   "source": [
    "# Selected winning hyperparameters\n",
    "print(fitted_models['l1'].best_estimator_)\n",
    "print(fitted_models['l2'].best_estimator_)\n",
    "print(fitted_models['rf'].best_estimator_)\n",
    "print(fitted_models['gb'].best_estimator_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
